{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention from Scratch\n",
    "\n",
    "Implementing scaled dot-product attention and multi-head attention in PyTorch — the core building block behind every modern vision transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "now9kipevpo",
   "source": "## Overview\n\n**Builds:** `scaled_dot_product_attention` · `MultiHeadAttention`\n**Concepts:** Q/K/V projections · scaled dot-product · why √d_k · multi-head attention · head specialization\n**Requires:** nothing — standalone\n**References:** [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762) · [CS231N Lecture 11](https://cs231n.stanford.edu/slides/2024/lecture_11.pdf)\n**Output:** `assets/gradcam/attention_single_head.png` · `assets/gradcam/attention_multi_head.png`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 — Scaled Dot-Product Attention\n",
    "\n",
    "A transformer doesn't process each token in isolation. Every token looks at every other token simultaneously and decides how much to attend to each.\n",
    "\n",
    "Three learned projections of each input token:\n",
    "- **Query (Q)** — what this token is looking for\n",
    "- **Key (K)** — what each token offers\n",
    "- **Value (V)** — the content to aggregate\n",
    "\n",
    "The attention score between two tokens is their Q·K dot product, scaled by √d_k. Softmax turns scores into weights. The output is a weighted sum of V vectors.\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "**Why scale by √d_k?** With large d_k, dot products grow large and push softmax into flat regions with tiny gradients — the model stops learning. Scaling by √d_k keeps the variance of dot products at ~1 regardless of dimension size. ([Vaswani et al., 2017 — Section 3.2](https://arxiv.org/abs/1706.03762))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "        q: Query  — shape (..., seq_len, d_k)\n",
    "        k: Key    — shape (..., seq_len, d_k)\n",
    "        v: Value  — shape (..., seq_len, d_v)\n",
    "        mask: Optional boolean mask — shape (..., seq_len, seq_len)\n",
    "\n",
    "    Returns:\n",
    "        output:       shape (..., seq_len, d_v)\n",
    "        attn_weights: shape (..., seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    d_k = q.size(-1)\n",
    "\n",
    "    # Step 1: QK^T — how much each token attends to every other token\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1))       # (..., seq, seq)\n",
    "\n",
    "    # Step 2: Scale — prevents large dot products from saturating softmax\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "\n",
    "    # Step 3: Optional mask (used in decoders to block future tokens)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    # Step 4: Softmax — convert scores to weights that sum to 1\n",
    "    attn_weights = F.softmax(scores, dim=-1)             # (..., seq, seq)\n",
    "\n",
    "    # Step 5: Weighted sum of V\n",
    "    output = torch.matmul(attn_weights, v)               # (..., seq, d_v)\n",
    "\n",
    "    return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify shapes\n",
    "batch_size = 2\n",
    "seq_len    = 6\n",
    "d_k        = 64\n",
    "\n",
    "q = torch.randn(batch_size, seq_len, d_k)\n",
    "k = torch.randn(batch_size, seq_len, d_k)\n",
    "v = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "print(f\"Q shape:              {q.shape}\")\n",
    "print(f\"K shape:              {k.shape}\")\n",
    "print(f\"V shape:              {v.shape}\")\n",
    "print(f\"Output shape:         {output.shape}\")          # (2, 6, 64)\n",
    "print(f\"Attn weights shape:   {attn_weights.shape}\")    # (2, 6, 6)\n",
    "print(f\"Weights sum to 1:     {attn_weights[0].sum(dim=-1)}\")  # all 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    im = ax.imshow(attn_weights[i].detach().numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Attention weights — batch {i}')\n",
    "    ax.set_xlabel('Key position')\n",
    "    ax.set_ylabel('Query position')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle('Scaled dot-product attention (random weights, untrained)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../assets/gradcam/attention_single_head.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 — Multi-Head Attention\n",
    "\n",
    "A single attention head learns one way of relating tokens. Multi-head attention runs `h` attention operations in **parallel**, each in a lower-dimensional subspace (`d_k = d_model / h`).\n",
    "\n",
    "Each head can specialize: one might capture local relationships, another long-range structure. The outputs of all heads are concatenated and projected back to `d_model`.\n",
    "\n",
    "**Shape flow:**\n",
    "```\n",
    "Input:            (batch, seq_len, d_model)\n",
    "Project Q, K, V:  (batch, seq_len, d_model)  via W_q, W_k, W_v\n",
    "Split into heads: (batch, num_heads, seq_len, d_k)  where d_k = d_model / num_heads\n",
    "Attention:        (batch, num_heads, seq_len, d_k)\n",
    "Merge heads:      (batch, seq_len, d_model)\n",
    "Output project:   (batch, seq_len, d_model)  via W_o\n",
    "```\n",
    "\n",
    "([Vaswani et al., 2017 — Section 3.3](https://arxiv.org/abs/1706.03762) · [CS231N Lecture 11](https://cs231n.stanford.edu/slides/2024/lecture_11.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model    = d_model\n",
    "        self.num_heads  = num_heads\n",
    "        self.d_k        = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"(batch, seq_len, d_model) -> (batch, num_heads, seq_len, d_k)\"\"\"\n",
    "        batch, seq_len, _ = x.shape\n",
    "        x = x.view(batch, seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch, seq_len, _ = x.shape\n",
    "\n",
    "        # Project and split\n",
    "        q = self.split_heads(self.W_q(x))   # (batch, heads, seq_len, d_k)\n",
    "        k = self.split_heads(self.W_k(x))\n",
    "        v = self.split_heads(self.W_v(x))\n",
    "\n",
    "        # Attention across all heads\n",
    "        attn_out, attn_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        # attn_out: (batch, heads, seq_len, d_k)\n",
    "\n",
    "        # Merge heads\n",
    "        attn_out = attn_out.transpose(1, 2).reshape(batch, seq_len, self.d_model)\n",
    "\n",
    "        # Output projection\n",
    "        output = self.W_o(attn_out)          # (batch, seq_len, d_model)\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify shapes\n",
    "d_model    = 512\n",
    "num_heads  = 8\n",
    "seq_len    = 10\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "x   = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attn_weights = mha(x)\n",
    "\n",
    "print(f\"Input shape:          {x.shape}\")             # (2, 10, 512)\n",
    "print(f\"Output shape:         {output.shape}\")         # (2, 10, 512)\n",
    "print(f\"Attn weights shape:   {attn_weights.shape}\")   # (2, 8, 10, 10)\n",
    "print(f\"d_k per head:         {d_model} / {num_heads} = {d_model // num_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vj3bg9sua2q",
   "source": "## Shape Summary\n\n| Tensor | Shape | Notes |\n|---|---|---|\n| Q, K, V (single head) | `(batch, seq_len, d_k)` | d_k = 64 in this notebook |\n| Attention scores | `(batch, seq_len, seq_len)` | before softmax |\n| Attention weights | `(batch, seq_len, seq_len)` | after softmax — rows sum to 1 |\n| Attention output | `(batch, seq_len, d_k)` | weighted sum of V |\n| Q, K, V (multi-head) | `(batch, num_heads, seq_len, d_k)` | split along d_model |\n| MHA input / output | `(batch, seq_len, d_model)` | shape preserved — residual-friendly |\n| Attention weights (MHA) | `(batch, num_heads, seq_len, seq_len)` | one matrix per head |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize each head's attention pattern\n",
    "weights = attn_weights[0].detach()   # (num_heads, seq_len, seq_len)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 6))\n",
    "\n",
    "for head_idx, ax in enumerate(axes.flat):\n",
    "    im = ax.imshow(weights[head_idx].numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Head {head_idx + 1}')\n",
    "    ax.set_xlabel('Key pos')\n",
    "    ax.set_ylabel('Query pos')\n",
    "\n",
    "plt.suptitle('Multi-head attention — weights per head (random weights, untrained)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../assets/gradcam/attention_multi_head.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Uniform attention is expected with random weights.\")\n",
    "print(\"After training on image patches, heads specialize to attend to different spatial regions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codingenv",
   "language": "python",
   "name": "codingenv"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}