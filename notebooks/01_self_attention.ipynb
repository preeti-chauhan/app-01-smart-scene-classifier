{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention from Scratch\n",
    "\n",
    "Implementing scaled dot-product attention and multi-head attention in PyTorch — the core building block behind every modern vision transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "now9kipevpo",
   "source": "## Overview\n\n**Builds:** `scaled_dot_product_attention` · `MultiHeadAttention`\n**Concepts:** Q/K/V projections · scaled dot-product · why √d_k · multi-head attention · head specialization\n**Requires:** nothing — standalone\n**References:** [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762) · [CS231N Lecture 11](https://cs231n.stanford.edu/slides/2024/lecture_11.pdf)\n**Output:** `assets/gradcam/attention_single_head.png` · `assets/gradcam/attention_multi_head.png`\n\n---\n\n**Contents**\n1. [Part 1 — Scaled Dot-Product Attention](#part-1--scaled-dot-product-attention)\n2. [Part 2 — Multi-Head Attention](#part-2--multi-head-attention)\n3. [Shape Summary](#shape-summary)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 — Scaled Dot-Product Attention\n",
    "\n",
    "A transformer doesn't process each token in isolation. Every token looks at every other token simultaneously and decides how much to attend to each.\n",
    "\n",
    "Three learned projections of each input token:\n",
    "- **Query (Q)** — what this token is looking for\n",
    "- **Key (K)** — what each token offers\n",
    "- **Value (V)** — the content to aggregate\n",
    "\n",
    "The attention score between two tokens is their Q·K dot product, scaled by √d_k. Softmax turns scores into weights. The output is a weighted sum of V vectors.\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "**Why scale by √d_k?** With large d_k, dot products grow large and push softmax into flat regions with tiny gradients — the model stops learning. Scaling by √d_k keeps the variance of dot products at ~1 regardless of dimension size. ([Vaswani et al., 2017 — Section 3.2](https://arxiv.org/abs/1706.03762))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify shapes\nbatch_size = 2\nseq_len    = 6\nd_k        = 64\n\nq = torch.randn(batch_size, seq_len, d_k)\nk = torch.randn(batch_size, seq_len, d_k)\nv = torch.randn(batch_size, seq_len, d_k)\n\noutput, attn_weights = scaled_dot_product_attention(q, k, v)\n\nprint(f\"Q shape:              {q.shape}\")\nprint(f\"K shape:              {k.shape}\")\nprint(f\"V shape:              {v.shape}\")\nprint(f\"Output shape:         {output.shape}\")\nprint(f\"Attn weights shape:   {attn_weights.shape}\")\nprint(f\"Weights sum to 1:     {attn_weights[0].sum(dim=-1)}\")\n\nassert output.shape == (batch_size, seq_len, d_k)\nassert attn_weights.shape == (batch_size, seq_len, seq_len)\nassert torch.allclose(attn_weights[0].sum(dim=-1), torch.ones(seq_len), atol=1e-6)\nprint(\"All assertions passed.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify shapes\n",
    "batch_size = 2\n",
    "seq_len    = 6\n",
    "d_k        = 64\n",
    "\n",
    "q = torch.randn(batch_size, seq_len, d_k)\n",
    "k = torch.randn(batch_size, seq_len, d_k)\n",
    "v = torch.randn(batch_size, seq_len, d_k)\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "print(f\"Q shape:              {q.shape}\")\n",
    "print(f\"K shape:              {k.shape}\")\n",
    "print(f\"V shape:              {v.shape}\")\n",
    "print(f\"Output shape:         {output.shape}\")          # (2, 6, 64)\n",
    "print(f\"Attn weights shape:   {attn_weights.shape}\")    # (2, 6, 6)\n",
    "print(f\"Weights sum to 1:     {attn_weights[0].sum(dim=-1)}\")  # all 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    im = ax.imshow(attn_weights[i].detach().numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Attention weights — batch {i}')\n",
    "    ax.set_xlabel('Key position')\n",
    "    ax.set_ylabel('Query position')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle('Scaled dot-product attention (random weights, untrained)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../assets/gradcam/attention_single_head.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6t3g1lfpoqa",
   "source": "---\n[Back to top](#overview)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Verify shapes\nd_model    = 512\nnum_heads  = 8\nseq_len    = 10\nbatch_size = 2\n\nmha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\nx   = torch.randn(batch_size, seq_len, d_model)\n\noutput, attn_weights = mha(x)\n\nprint(f\"Input shape:          {x.shape}\")\nprint(f\"Output shape:         {output.shape}\")\nprint(f\"Attn weights shape:   {attn_weights.shape}\")\nprint(f\"d_k per head:         {d_model} / {num_heads} = {d_model // num_heads}\")\n\nassert output.shape == (batch_size, seq_len, d_model)\nassert attn_weights.shape == (batch_size, num_heads, seq_len, seq_len)\nassert output.shape == x.shape, \"MHA must preserve input shape for residual connections\"\nprint(\"All assertions passed.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "nrhu654qtj",
   "source": "---\n[Back to top](#overview)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model    = d_model\n",
    "        self.num_heads  = num_heads\n",
    "        self.d_k        = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"(batch, seq_len, d_model) -> (batch, num_heads, seq_len, d_k)\"\"\"\n",
    "        batch, seq_len, _ = x.shape\n",
    "        x = x.view(batch, seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch, seq_len, _ = x.shape\n",
    "\n",
    "        # Project and split\n",
    "        q = self.split_heads(self.W_q(x))   # (batch, heads, seq_len, d_k)\n",
    "        k = self.split_heads(self.W_k(x))\n",
    "        v = self.split_heads(self.W_v(x))\n",
    "\n",
    "        # Attention across all heads\n",
    "        attn_out, attn_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        # attn_out: (batch, heads, seq_len, d_k)\n",
    "\n",
    "        # Merge heads\n",
    "        attn_out = attn_out.transpose(1, 2).reshape(batch, seq_len, self.d_model)\n",
    "\n",
    "        # Output projection\n",
    "        output = self.W_o(attn_out)          # (batch, seq_len, d_model)\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify shapes\n",
    "d_model    = 512\n",
    "num_heads  = 8\n",
    "seq_len    = 10\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "x   = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attn_weights = mha(x)\n",
    "\n",
    "print(f\"Input shape:          {x.shape}\")             # (2, 10, 512)\n",
    "print(f\"Output shape:         {output.shape}\")         # (2, 10, 512)\n",
    "print(f\"Attn weights shape:   {attn_weights.shape}\")   # (2, 8, 10, 10)\n",
    "print(f\"d_k per head:         {d_model} / {num_heads} = {d_model // num_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vj3bg9sua2q",
   "source": "## Shape Summary\n\n| Tensor | Shape | Notes |\n|---|---|---|\n| Q, K, V (single head) | `(batch, seq_len, d_k)` | d_k = 64 in this notebook |\n| Attention scores | `(batch, seq_len, seq_len)` | before softmax |\n| Attention weights | `(batch, seq_len, seq_len)` | after softmax — rows sum to 1 |\n| Attention output | `(batch, seq_len, d_k)` | weighted sum of V |\n| Q, K, V (multi-head) | `(batch, num_heads, seq_len, d_k)` | split along d_model |\n| MHA input / output | `(batch, seq_len, d_model)` | shape preserved — residual-friendly |\n| Attention weights (MHA) | `(batch, num_heads, seq_len, seq_len)` | one matrix per head |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "0sup8r87d3xr",
   "source": "---\n**Next:** [`02_vit_from_scratch.ipynb`](02_vit_from_scratch.ipynb) — apply this attention mechanism to image patches to build a full Vision Transformer",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize each head's attention pattern\n",
    "weights = attn_weights[0].detach()   # (num_heads, seq_len, seq_len)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 6))\n",
    "\n",
    "for head_idx, ax in enumerate(axes.flat):\n",
    "    im = ax.imshow(weights[head_idx].numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Head {head_idx + 1}')\n",
    "    ax.set_xlabel('Key pos')\n",
    "    ax.set_ylabel('Query pos')\n",
    "\n",
    "plt.suptitle('Multi-head attention — weights per head (random weights, untrained)', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../assets/gradcam/attention_multi_head.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Uniform attention is expected with random weights.\")\n",
    "print(\"After training on image patches, heads specialize to attend to different spatial regions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codingenv",
   "language": "python",
   "name": "codingenv"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}