{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) from Scratch\n",
    "\n",
    "Building the full ViT architecture in PyTorch — patch embeddings, CLS token, positional encoding, and stacked transformer encoder blocks. Uses the `MultiHeadAttention` built in notebook 01.\n",
    "\n",
    "Reference: [An Image is Worth 16×16 Words — Dosovitskiy et al., 2020](https://arxiv.org/abs/2010.11929) · [CS231N Lecture 12](https://cs231n.stanford.edu/slides/2024/lecture_12.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ok4hmm71eub",
   "source": "## Overview\n\n**Builds:** `PatchEmbedding` · `TransformerEncoderBlock` · `VisionTransformer`\n**Concepts:** patch embeddings · Conv2d trick · CLS token · positional encoding · pre-norm · ViT-B/16\n**Requires:** `01_self_attention.ipynb` — uses `MultiHeadAttention` built there\n**References:** [Dosovitskiy et al., 2020](https://arxiv.org/abs/2010.11929) · [CS231N Lecture 12](https://cs231n.stanford.edu/slides/2024/lecture_12.pdf)\n**Output:** `assets/gradcam/patch_grid.png` · `assets/gradcam/cls_attention_map.png`",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from einops import rearrange\n",
    "\n",
    "matplotlib.rcParams['figure.dpi'] = 120\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup — MultiHeadAttention from Notebook 01\n",
    "\n",
    "Copied here so this notebook is self-contained. Built from scratch in `01_self_attention.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    d_k = q.size(-1)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    return torch.matmul(attn_weights, v), attn_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k       = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model   = d_model\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        return x.view(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, _ = x.shape\n",
    "        q, k, v = self.split_heads(self.W_q(x)), self.split_heads(self.W_k(x)), self.split_heads(self.W_v(x))\n",
    "        out, attn = scaled_dot_product_attention(q, k, v, mask)\n",
    "        out = out.transpose(1, 2).reshape(B, T, self.d_model)\n",
    "        return self.W_o(out), attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 — Patch Embedding\n",
    "\n",
    "ViT treats an image as a sequence of fixed-size patches. A 224×224 image with 16×16 patches produces **(224/16)² = 196 patch tokens** — the sequence the transformer operates on.\n",
    "\n",
    "**The Conv2d trick:** Instead of slicing patches manually and flattening, a single `Conv2d` with `kernel_size=patch_size` and `stride=patch_size` does the same thing in one GPU-efficient operation. Each filter learns one dimension of the patch projection.\n",
    "\n",
    "```\n",
    "Input:   (B, 3, 224, 224)\n",
    "Conv2d:  (B, 768, 14, 14)   ← 14 = 224/16\n",
    "Flatten: (B, 768, 196)\n",
    "Permute: (B, 196, 768)       ← 196 tokens, each 768-dim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=16, in_channels=3, d_model=768):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        # Conv2d trick: kernel and stride = patch_size → each kernel covers exactly one patch\n",
    "        self.proj = nn.Conv2d(in_channels, d_model, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)             # (B, d_model, H/P, W/P)\n",
    "        x = x.flatten(2)             # (B, d_model, num_patches)\n",
    "        x = x.transpose(1, 2)        # (B, num_patches, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify patch embedding shapes\n",
    "B, C, H, W = 2, 3, 224, 224\n",
    "patch_size = 16\n",
    "d_model    = 768\n",
    "\n",
    "x   = torch.randn(B, C, H, W)\n",
    "pe  = PatchEmbedding(image_size=H, patch_size=patch_size, d_model=d_model)\n",
    "out = pe(x)\n",
    "\n",
    "print(f\"Input image:       {x.shape}\")\n",
    "print(f\"After Conv2d:      {pe.proj(x).shape}\")    # (2, 768, 14, 14)\n",
    "print(f\"Patch embeddings:  {out.shape}\")             # (2, 196, 768)\n",
    "print(f\"Number of patches: {pe.num_patches}\")        # 196 = 14×14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: show the 196 patches on the original image grid\n",
    "img = torch.rand(3, 224, 224)  # dummy image\n",
    "\n",
    "# Rearrange image into patches using einops\n",
    "patches = rearrange(img, 'c (h p1) (w p2) -> (h w) c p1 p2', p1=patch_size, p2=patch_size)\n",
    "\n",
    "fig, axes = plt.subplots(14, 14, figsize=(8, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(patches[i].permute(1, 2, 0).numpy())\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(f'224×224 image split into 196 patches of 16×16 each', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../assets/gradcam/patch_grid.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 — CLS Token and Positional Encoding\n",
    "\n",
    "**CLS token:** A learned vector prepended to the patch sequence. It has no corresponding image region — its role is to aggregate information from all patches through attention. After the final transformer block, only the CLS token's output is passed to the classifier.\n",
    "\n",
    "**Why not average all patch outputs?** Averaging loses the ability to weight patches differently. The CLS token learns, through attention, to selectively aggregate what matters for classification.\n",
    "\n",
    "**Positional encoding:** Transformers have no notion of order — attention is permutation-invariant. Without positional encoding, the model treats patch token 0 (top-left) the same as patch token 195 (bottom-right). ViT uses learned positional embeddings, one per position (197 total: 1 CLS + 196 patches).\n",
    "\n",
    "```\n",
    "Patches:      (B, 196, 768)\n",
    "CLS prepend:  (B, 197, 768)\n",
    "Pos encoding: (B, 197, 768)  ← added, not concatenated\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate CLS token + positional encoding\n",
    "num_patches = 196\n",
    "d_model     = 768\n",
    "B           = 2\n",
    "\n",
    "patch_tokens = torch.randn(B, num_patches, d_model)\n",
    "\n",
    "# CLS token: one learnable vector, expanded across batch\n",
    "cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "cls_expanded = cls_token.expand(B, -1, -1)             # (B, 1, 768)\n",
    "tokens = torch.cat([cls_expanded, patch_tokens], dim=1) # (B, 197, 768)\n",
    "\n",
    "# Positional encoding: one per position, learned\n",
    "pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, d_model))\n",
    "tokens = tokens + pos_embed                             # (B, 197, 768)\n",
    "\n",
    "print(f\"Patch tokens:       {patch_tokens.shape}\")\n",
    "print(f\"After CLS prepend:  {torch.cat([cls_expanded, patch_tokens], dim=1).shape}\")\n",
    "print(f\"After pos encoding: {tokens.shape}\")\n",
    "print(f\"\\nCLS token is position 0. Patch tokens are positions 1-196.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 — Transformer Encoder Block\n",
    "\n",
    "Each block applies two operations, each with a residual connection:\n",
    "\n",
    "```\n",
    "x = x + MultiHeadAttention(LayerNorm(x))   ← attention + residual\n",
    "x = x + MLP(LayerNorm(x))                  ← feed-forward + residual\n",
    "```\n",
    "\n",
    "**Pre-norm vs post-norm:** ViT uses LayerNorm *before* each sub-layer (pre-norm), not after. Pre-norm stabilizes training in deep transformers.\n",
    "\n",
    "**MLP:** Two linear layers with GELU activation. Hidden dim = `d_model × mlp_ratio` (typically 4×). For ViT-B: 768 → 3072 → 768.\n",
    "\n",
    "**GELU vs ReLU:** GELU is smoother near zero and empirically works better in transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1   = nn.LayerNorm(d_model)\n",
    "        self.attn    = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm2   = nn.LayerNorm(d_model)\n",
    "        self.mlp     = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * mlp_ratio),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * mlp_ratio, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention sub-layer with pre-norm and residual\n",
    "        attn_out, attn_weights = self.attn(self.norm1(x))\n",
    "        x = x + self.dropout(attn_out)\n",
    "\n",
    "        # MLP sub-layer with pre-norm and residual\n",
    "        x = x + self.dropout(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify encoder block shapes\n",
    "block = TransformerEncoderBlock(d_model=768, num_heads=12)\n",
    "x     = torch.randn(2, 197, 768)  # (batch, 197 tokens, 768 dim)\n",
    "\n",
    "out, attn = block(x)\n",
    "print(f\"Input:        {x.shape}\")\n",
    "print(f\"Output:       {out.shape}\")   # same shape — residual connection preserves it\n",
    "print(f\"Attn weights: {attn.shape}\")  # (2, 12 heads, 197, 197)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 — Full Vision Transformer\n",
    "\n",
    "Stacking everything: patch embedding → CLS + positional encoding → N encoder blocks → final LayerNorm → CLS token → classifier.\n",
    "\n",
    "**ViT-B/16 configuration (what we build):**\n",
    "- `image_size=224, patch_size=16` → 196 patches\n",
    "- `d_model=768, num_heads=12, num_layers=12`\n",
    "- `mlp_ratio=4` → MLP hidden dim = 3072\n",
    "- ~86M parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=16, in_channels=3,\n",
    "                 num_classes=10, d_model=768, num_heads=12, num_layers=12,\n",
    "                 mlp_ratio=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        self.patch_embed = PatchEmbedding(image_size, patch_size, in_channels, d_model)\n",
    "        self.cls_token   = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.pos_embed   = nn.Parameter(torch.zeros(1, num_patches + 1, d_model))\n",
    "        self.dropout     = nn.Dropout(dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerEncoderBlock(d_model, num_heads, mlp_ratio, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, num_classes)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "\n",
    "        # 1. Patch embedding\n",
    "        x = self.patch_embed(x)                          # (B, 196, 768)\n",
    "\n",
    "        # 2. Prepend CLS token\n",
    "        cls = self.cls_token.expand(B, -1, -1)           # (B, 1, 768)\n",
    "        x   = torch.cat([cls, x], dim=1)                 # (B, 197, 768)\n",
    "\n",
    "        # 3. Add positional encoding\n",
    "        x = self.dropout(x + self.pos_embed)             # (B, 197, 768)\n",
    "\n",
    "        # 4. Transformer encoder blocks\n",
    "        attn_weights_all = []\n",
    "        for block in self.blocks:\n",
    "            x, attn = block(x)\n",
    "            attn_weights_all.append(attn)\n",
    "\n",
    "        # 5. Final norm\n",
    "        x = self.norm(x)                                 # (B, 197, 768)\n",
    "\n",
    "        # 6. CLS token → classifier\n",
    "        logits = self.head(x[:, 0])                      # (B, num_classes)\n",
    "\n",
    "        return logits, attn_weights_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass — verify every shape\n",
    "vit = VisionTransformer(\n",
    "    image_size=224, patch_size=16, in_channels=3,\n",
    "    num_classes=10, d_model=768, num_heads=12, num_layers=12\n",
    ")\n",
    "\n",
    "x = torch.randn(2, 3, 224, 224)  # batch of 2 images\n",
    "logits, attn_weights_all = vit(x)\n",
    "\n",
    "print(f\"Input:                     {x.shape}\")\n",
    "print(f\"Output logits:             {logits.shape}\")                     # (2, 10)\n",
    "print(f\"Number of encoder blocks:  {len(attn_weights_all)}\")\n",
    "print(f\"Attn weights per block:    {attn_weights_all[0].shape}\")        # (2, 12, 197, 197)\n",
    "print(f\"\\nCLS token attends to 196 patches + itself = 197 positions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter count\n",
    "total = sum(p.numel() for p in vit.parameters())\n",
    "trainable = sum(p.numel() for p in vit.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters:     {total:,}\")\n",
    "print(f\"Trainable parameters: {trainable:,}\")\n",
    "print(f\"Model size (FP32):    {total * 4 / 1e6:.1f} MB\")\n",
    "print(f\"\\nViT-B/16 reference:   86M parameters — expected range: 85-87M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dh3zd6kczg7",
   "source": "## Shape Summary\n\n| Tensor | Shape | Notes |\n|---|---|---|\n| Input image | `(B, 3, 224, 224)` | — |\n| After Conv2d (patch embed) | `(B, 768, 14, 14)` | 14 = 224 / 16 |\n| Patch tokens | `(B, 196, 768)` | 196 = 14×14 patches |\n| After CLS prepend | `(B, 197, 768)` | position 0 = CLS token |\n| After positional encoding | `(B, 197, 768)` | same shape — values shifted |\n| Encoder block output | `(B, 197, 768)` | residuals preserve shape through all 12 blocks |\n| Attention weights per block | `(B, 12, 197, 197)` | 12 heads, each token attends to 197 positions |\n| Final CLS token | `(B, 768)` | `x[:, 0]` — only this goes to classifier |\n| Logits | `(B, 10)` | 10 scene classes (SUN397 subset) |\n| Parameters (ViT-B/16) | ~86M | patch_embed + pos_embed + 12 blocks + head |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CLS token attention from the last block\n",
    "# Row 0 = how CLS token attends to all 196 patches\n",
    "last_attn = attn_weights_all[-1][0].detach()  # (12 heads, 197, 197)\n",
    "cls_attn  = last_attn[:, 0, 1:]               # (12, 196) — CLS row, patch columns only\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(14, 10))\n",
    "for h, ax in enumerate(axes.flat):\n",
    "    attn_map = cls_attn[h].reshape(14, 14).numpy()\n",
    "    im = ax.imshow(attn_map, cmap='hot')\n",
    "    ax.set_title(f'Head {h+1} — CLS attention')\n",
    "    ax.axis('off')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle('CLS token attention to 14×14 patch grid (last block, random weights)', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../assets/gradcam/cls_attention_map.png', bbox_inches='tight', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"Uniform attention expected with random weights.\")\n",
    "print(\"After training, heads specialize to attend to class-relevant regions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codingenv",
   "language": "python",
   "name": "codingenv"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}